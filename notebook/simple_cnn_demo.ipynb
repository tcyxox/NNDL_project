{
 "cells": [
  {
   "cell_type": "code",
   "id": "198c699a-e1e8-4f8b-8cd5-98a1d05f7ec3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T06:01:15.360264Z",
     "start_time": "2025-12-09T06:01:09.392720Z"
    }
   },
   "source": [
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, BatchSampler, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "c370d643-46fd-4d03-bb17-a875e79d5e2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T06:06:45.496568Z",
     "start_time": "2025-12-09T06:06:45.470568Z"
    }
   },
   "source": [
    "# Create Dataset class for multilabel classification\n",
    "class MultiClassImageDataset(Dataset):\n",
    "    def __init__(self, ann_df, super_map_df, sub_map_df, img_dir, transform=None):\n",
    "        self.ann_df = ann_df \n",
    "        self.super_map_df = super_map_df\n",
    "        self.sub_map_df = sub_map_df\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ann_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.ann_df['image'][idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        super_idx = self.ann_df['superclass_index'][idx]\n",
    "        super_label = self.super_map_df['class'][super_idx]\n",
    "        \n",
    "        sub_idx = self.ann_df['subclass_index'][idx]\n",
    "        sub_label = self.sub_map_df['class'][sub_idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)  \n",
    "            \n",
    "        return image, super_idx, super_label, sub_idx, sub_label\n",
    "\n",
    "class MultiClassImageTestDataset(Dataset):\n",
    "    def __init__(self, super_map_df, sub_map_df, img_dir, transform=None):\n",
    "        self.super_map_df = super_map_df\n",
    "        self.sub_map_df = sub_map_df\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self): # Count files in img_dir\n",
    "        return len([fname for fname in os.listdir(self.img_dir)])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = str(idx) + '.jpg'\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)  \n",
    "            \n",
    "        return image, img_name"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "e7398553-8842-4ad8-b348-767921a22482",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T06:16:53.524332Z",
     "start_time": "2025-12-09T06:16:53.496350Z"
    }
   },
   "source": [
    "train_ann_df = pd.read_csv('data/train_data.csv')\n",
    "# test_ann_df = pd.read_csv('data/test_data.csv')\n",
    "super_map_df = pd.read_csv('data/superclass_mapping.csv')\n",
    "sub_map_df = pd.read_csv('data/subclass_mapping.csv')\n",
    "\n",
    "# train_img_dir = 'train_shuffle'\n",
    "# test_img_dir = 'test_shuffle'\n",
    "train_img_dir = 'data/train_images'\n",
    "test_img_dir = 'data/test_images'\n",
    "\n",
    "image_preprocessing = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0), std=(1)),\n",
    "])\n",
    "\n",
    "# Create train and val split\n",
    "train_dataset = MultiClassImageDataset(train_ann_df, super_map_df, sub_map_df, train_img_dir, transform=image_preprocessing)\n",
    "train_dataset, val_dataset = random_split(train_dataset, [0.9, 0.1]) \n",
    "\n",
    "# Create test dataset\n",
    "test_dataset = MultiClassImageTestDataset(super_map_df, sub_map_df, test_img_dir, transform=image_preprocessing)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, \n",
    "                        batch_size=batch_size, \n",
    "                        shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                         batch_size=1, \n",
    "                         shuffle=False)"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T06:16:56.415659Z",
     "start_time": "2025-12-09T06:16:56.365882Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[0.4824, 0.4784, 0.4706,  ..., 0.5137, 0.5059, 0.5020],\n",
      "         [0.5255, 0.5216, 0.5137,  ..., 0.5412, 0.5294, 0.5216],\n",
      "         [0.5294, 0.5333, 0.5255,  ..., 0.5882, 0.5686, 0.5569],\n",
      "         ...,\n",
      "         [0.7765, 0.7804, 0.7922,  ..., 0.8000, 0.8118, 0.7882],\n",
      "         [0.7686, 0.7765, 0.7843,  ..., 0.8275, 0.8392, 0.8157],\n",
      "         [0.7843, 0.7882, 0.7961,  ..., 0.8196, 0.8353, 0.8118]],\n",
      "\n",
      "        [[0.3882, 0.3843, 0.3765,  ..., 0.4314, 0.4235, 0.4196],\n",
      "         [0.4275, 0.4275, 0.4157,  ..., 0.4588, 0.4471, 0.4392],\n",
      "         [0.4431, 0.4471, 0.4392,  ..., 0.4941, 0.4745, 0.4627],\n",
      "         ...,\n",
      "         [0.7373, 0.7412, 0.7529,  ..., 0.7725, 0.7843, 0.7608],\n",
      "         [0.7333, 0.7412, 0.7490,  ..., 0.7961, 0.8078, 0.7843],\n",
      "         [0.7490, 0.7529, 0.7608,  ..., 0.7882, 0.8039, 0.7804]],\n",
      "\n",
      "        [[0.2941, 0.2902, 0.2824,  ..., 0.3255, 0.3176, 0.3137],\n",
      "         [0.3412, 0.3333, 0.3294,  ..., 0.3529, 0.3412, 0.3333],\n",
      "         [0.3608, 0.3569, 0.3569,  ..., 0.4000, 0.3804, 0.3686],\n",
      "         ...,\n",
      "         [0.7020, 0.7020, 0.7176,  ..., 0.7490, 0.7608, 0.7373],\n",
      "         [0.7059, 0.7059, 0.7216,  ..., 0.7843, 0.7961, 0.7725],\n",
      "         [0.7216, 0.7255, 0.7333,  ..., 0.7765, 0.7961, 0.7725]]]), 0, 'bird', 40, 'red-backed sandpiper, dunlin, Erolia alpina')\n"
     ]
    }
   ],
   "execution_count": 12,
   "source": "print(train_dataset[0])",
   "id": "c813fe47395dc971"
  },
  {
   "cell_type": "code",
   "id": "bf33a131-0c66-40dc-b8d4-ba5d0f840840",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T06:17:30.174764Z",
     "start_time": "2025-12-09T06:17:30.138182Z"
    }
   },
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.feature_size = input_size // (2**3)\n",
    "        \n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, 32, 3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, 32, 3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 64, 3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 64, 3, padding='same'), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(128, 128, 3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(128, 128, 3, padding='same'), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.feature_size * self.feature_size * 128, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3a = nn.Linear(128, 4)\n",
    "        self.fc3b = nn.Linear(128, 88)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        \n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        super_out = self.fc3a(x)\n",
    "        sub_out = self.fc3b(x)\n",
    "        return super_out, sub_out\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, model, criterion, optimizer, train_loader, val_loader, test_loader=None, device='cuda'):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "\n",
    "    def train_epoch(self):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(self.train_loader):\n",
    "            inputs, super_labels, sub_labels = data[0].to(device), data[1].to(device), data[3].to(device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            super_outputs, sub_outputs = self.model(inputs)\n",
    "            loss = self.criterion(super_outputs, super_labels) + self.criterion(sub_outputs, sub_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f'Training loss: {running_loss/i:.3f}')\n",
    "\n",
    "    def validate_epoch(self):\n",
    "        super_correct = 0\n",
    "        sub_correct = 0\n",
    "        total = 0\n",
    "        running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(self.val_loader):\n",
    "                inputs, super_labels, sub_labels = data[0].to(device), data[1].to(device), data[3].to(device)\n",
    "\n",
    "                super_outputs, sub_outputs = self.model(inputs)\n",
    "                loss = self.criterion(super_outputs, super_labels) + self.criterion(sub_outputs, sub_labels)\n",
    "                _, super_predicted = torch.max(super_outputs.data, 1)\n",
    "                _, sub_predicted = torch.max(sub_outputs.data, 1)\n",
    "                \n",
    "                total += super_labels.size(0)\n",
    "                super_correct += (super_predicted == super_labels).sum().item()\n",
    "                sub_correct += (sub_predicted == sub_labels).sum().item()\n",
    "                running_loss += loss.item()            \n",
    "\n",
    "        print(f'Validation loss: {running_loss/i:.3f}')\n",
    "        print(f'Validation superclass acc: {100 * super_correct / total:.2f} %')\n",
    "        print(f'Validation subclass acc: {100 * sub_correct / total:.2f} %') \n",
    "\n",
    "    def test(self, save_to_csv=False, return_predictions=False):\n",
    "        if not self.test_loader:\n",
    "            raise NotImplementedError('test_loader not specified')\n",
    "\n",
    "        # Evaluate on test set, in this simple demo no special care is taken for novel/unseen classes\n",
    "        test_predictions = {'image': [], 'superclass_index': [], 'subclass_index': []}\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(self.test_loader):\n",
    "                inputs, img_name = data[0].to(device), data[1]\n",
    "        \n",
    "                super_outputs, sub_outputs = self.model(inputs)\n",
    "                _, super_predicted = torch.max(super_outputs.data, 1)\n",
    "                _, sub_predicted = torch.max(sub_outputs.data, 1)\n",
    "                \n",
    "                test_predictions['image'].append(img_name[0])\n",
    "                test_predictions['superclass_index'].append(super_predicted.item())\n",
    "                test_predictions['subclass_index'].append(sub_predicted.item())\n",
    "                \n",
    "        test_predictions = pd.DataFrame(data=test_predictions)\n",
    "        \n",
    "        if save_to_csv:\n",
    "            test_predictions.to_csv('example_test_predictions.csv', index=False)\n",
    "\n",
    "        if return_predictions:\n",
    "            return test_predictions"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "ebdf524a-98bf-4d0b-9b63-2b2b7b87daa1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T06:18:26.724206Z",
     "start_time": "2025-12-09T06:18:26.690124Z"
    }
   },
   "source": [
    "# Init model and trainer\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = CNN(input_size=64).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "trainer = Trainer(model, criterion, optimizer, train_loader, val_loader, test_loader)"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7941c289-d9b1-4714-b788-898b3b889f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Training loss: 4.238\n",
      "Validation loss: 3.783\n",
      "Validation superclass acc: 87.66 %\n",
      "Validation subclass acc: 19.15 %\n",
      "\n",
      "Epoch 2\n",
      "Training loss: 3.156\n",
      "Validation loss: 3.244\n",
      "Validation superclass acc: 88.61 %\n",
      "Validation subclass acc: 29.11 %\n",
      "\n",
      "Epoch 3\n",
      "Training loss: 2.588\n",
      "Validation loss: 2.851\n",
      "Validation superclass acc: 91.46 %\n",
      "Validation subclass acc: 34.65 %\n",
      "\n",
      "Epoch 4\n",
      "Training loss: 2.160\n",
      "Validation loss: 2.543\n",
      "Validation superclass acc: 89.56 %\n",
      "Validation subclass acc: 38.92 %\n",
      "\n",
      "Epoch 5\n",
      "Training loss: 1.780\n",
      "Validation loss: 2.336\n",
      "Validation superclass acc: 91.14 %\n",
      "Validation subclass acc: 49.37 %\n",
      "\n",
      "Epoch 6\n",
      "Training loss: 1.511\n",
      "Validation loss: 2.361\n",
      "Validation superclass acc: 90.66 %\n",
      "Validation subclass acc: 46.52 %\n",
      "\n",
      "Epoch 7\n",
      "Training loss: 1.214\n",
      "Validation loss: 2.422\n",
      "Validation superclass acc: 91.30 %\n",
      "Validation subclass acc: 48.89 %\n",
      "\n",
      "Epoch 8\n",
      "Training loss: 1.003\n",
      "Validation loss: 2.194\n",
      "Validation superclass acc: 91.14 %\n",
      "Validation subclass acc: 49.84 %\n",
      "\n",
      "Epoch 9\n",
      "Training loss: 0.778\n",
      "Validation loss: 2.287\n",
      "Validation superclass acc: 93.35 %\n",
      "Validation subclass acc: 53.32 %\n",
      "\n",
      "Epoch 10\n",
      "Training loss: 0.594\n",
      "Validation loss: 2.452\n",
      "Validation superclass acc: 93.51 %\n",
      "Validation subclass acc: 53.96 %\n",
      "\n",
      "Epoch 11\n",
      "Training loss: 0.464\n",
      "Validation loss: 2.700\n",
      "Validation superclass acc: 90.51 %\n",
      "Validation subclass acc: 50.95 %\n",
      "\n",
      "Epoch 12\n",
      "Training loss: 0.328\n",
      "Validation loss: 2.546\n",
      "Validation superclass acc: 93.35 %\n",
      "Validation subclass acc: 53.48 %\n",
      "\n",
      "Epoch 13\n",
      "Training loss: 0.201\n",
      "Validation loss: 2.698\n",
      "Validation superclass acc: 93.04 %\n",
      "Validation subclass acc: 54.11 %\n",
      "\n",
      "Epoch 14\n",
      "Training loss: 0.260\n",
      "Validation loss: 2.660\n",
      "Validation superclass acc: 92.09 %\n",
      "Validation subclass acc: 55.38 %\n",
      "\n",
      "Epoch 15\n",
      "Training loss: 0.226\n",
      "Validation loss: 2.854\n",
      "Validation superclass acc: 93.51 %\n",
      "Validation subclass acc: 53.01 %\n",
      "\n",
      "Epoch 16\n",
      "Training loss: 0.174\n",
      "Validation loss: 3.001\n",
      "Validation superclass acc: 92.25 %\n",
      "Validation subclass acc: 55.54 %\n",
      "\n",
      "Epoch 17\n",
      "Training loss: 0.147\n",
      "Validation loss: 3.403\n",
      "Validation superclass acc: 90.98 %\n",
      "Validation subclass acc: 52.06 %\n",
      "\n",
      "Epoch 18\n",
      "Training loss: 0.182\n",
      "Validation loss: 3.230\n",
      "Validation superclass acc: 93.04 %\n",
      "Validation subclass acc: 53.01 %\n",
      "\n",
      "Epoch 19\n",
      "Training loss: 0.111\n",
      "Validation loss: 3.183\n",
      "Validation superclass acc: 93.67 %\n",
      "Validation subclass acc: 53.32 %\n",
      "\n",
      "Epoch 20\n",
      "Training loss: 0.131\n",
      "Validation loss: 3.503\n",
      "Validation superclass acc: 90.82 %\n",
      "Validation subclass acc: 51.42 %\n",
      "\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(20):\n",
    "    print(f'Epoch {epoch+1}')\n",
    "    trainer.train_epoch()\n",
    "    trainer.validate_epoch()\n",
    "    print('')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d17e37-1a08-4ae1-8517-a16ff4769622",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab70fb9-6e14-49f1-b9bb-5f3da6807399",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698f0ba0-08ef-4093-a154-65d735174a25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d43947-4569-4920-929b-c997be9def58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61d77cbe-3ba1-46e9-9be1-208b9cabab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = trainer.test(save_to_csv=True, return_predictions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f4b681a-9e26-41f9-b0d1-4f1bc4077edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Superclass Accuracy\n",
      "Overall: 43.83 %\n",
      "Seen: 61.11 %\n",
      "Unseen: 0.00 %\n",
      "\n",
      "Subclass Accuracy\n",
      "Overall: 2.03 %\n",
      "Seen: 9.56 %\n",
      "Unseen: 0.00 %\n"
     ]
    }
   ],
   "source": [
    "# Quick script for evaluating generated csv files with ground truth\n",
    "\n",
    "super_correct = 0\n",
    "sub_correct = 0\n",
    "seen_super_correct = 0\n",
    "seen_sub_correct = 0\n",
    "unseen_super_correct = 0\n",
    "unseen_sub_correct = 0\n",
    "\n",
    "total = 0\n",
    "seen_super_total = 0\n",
    "unseen_super_total = 0\n",
    "seen_sub_total = 0\n",
    "unseen_sub_total = 0\n",
    "\n",
    "for i in range(len(test_predictions)):\n",
    "    super_pred = test_predictions['superclass_index'][i]\n",
    "    sub_pred = test_predictions['subclass_index'][i]\n",
    "\n",
    "    super_gt = test_ann_df['superclass_index'][i]\n",
    "    sub_gt = test_ann_df['subclass_index'][i]\n",
    "\n",
    "    # Total setting\n",
    "    if super_pred == super_gt:\n",
    "        super_correct += 1\n",
    "    if sub_pred == sub_gt:\n",
    "        sub_correct += 1\n",
    "    total += 1\n",
    "\n",
    "    # Unseen superclass setting\n",
    "    if super_gt == 3:\n",
    "        if super_pred == super_gt:\n",
    "            unseen_super_correct += 1\n",
    "        if sub_pred == sub_gt:\n",
    "            unseen_sub_correct += 1\n",
    "        unseen_super_total += 1\n",
    "        unseen_sub_total += 1\n",
    "    \n",
    "    # Seen superclass, unseen subclass setting\n",
    "    if super_gt != 3 and sub_gt == 87:\n",
    "        if super_pred == super_gt:\n",
    "            seen_super_correct += 1\n",
    "        if sub_pred == sub_gt:\n",
    "            unseen_sub_correct += 1\n",
    "        seen_super_total += 1\n",
    "        unseen_sub_total += 1\n",
    "\n",
    "    # Seen superclass and subclass setting\n",
    "    if super_gt != 3 and sub_gt != 87:\n",
    "        if super_pred == super_gt:\n",
    "            seen_super_correct += 1\n",
    "        if sub_pred == sub_gt:\n",
    "            seen_sub_correct += 1\n",
    "        seen_super_total += 1\n",
    "        seen_sub_total += 1\n",
    "\n",
    "print('Superclass Accuracy')\n",
    "print(f'Overall: {100*super_correct/total:.2f} %')\n",
    "print(f'Seen: {100*seen_super_correct/seen_super_total:.2f} %')\n",
    "print(f'Unseen: {100*unseen_super_correct/unseen_super_total:.2f} %')\n",
    "\n",
    "print('\\nSubclass Accuracy')\n",
    "print(f'Overall: {100*sub_correct/total:.2f} %')\n",
    "print(f'Seen: {100*seen_sub_correct/seen_sub_total:.2f} %')\n",
    "print(f'Unseen: {100*unseen_sub_correct/unseen_sub_total:.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed6dd55-94ab-41a9-b09c-08d7b1ae0fe6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
