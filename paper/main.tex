\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

\usepackage[final]{neurips2023}
\makeatletter
\renewcommand{\@noticestring}{}
\makeatother

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{amsfonts}

\title{Transfer Learning Project}
\author{%
  Chenyu Tian \\
  Columbia University\\
  \texttt{ct3308@columbia.edu} \\
  \And
  Yilu Yang \\
  Columbia University\\
  \texttt{yy3626@columbia.edu}
}

\begin{document}
\maketitle

\section{Introduction}
This project aims to solve a hierarchical classification problem by predicting both a superclass and its corresponding subclass for a given image. The problem has two primary complexities that test model robustness and generalization:

\textbf{Open Set Recognition (OSR):} The test set contains novel superclasses and subclasses not seen during training. The model must identify and reject these ``unknown'' samples as ``novel''.

\textbf{Distribution Shift:} The test set's class frequencies may differ from the training set. The model must generalize to this new statistical distribution of known classes.


\section{Related Work}
OSR contrasts with traditional closed-set recognition, which assumes all testing classes are known during training. The methods for solving OSR problems primarily fall into the following two groups~\cite{sun2023survey}.

\textbf{Discriminative Models:} Discriminative models aim to learn decision rules directly. This is often approached either by learning highly compact and discriminative representations for known classes (through score-based, distance-based or reconstruction-based methods) or by explicitly introducing "unknown" information into the training process. This unknown information can be synthesized from known classes.

\textbf{Generative Models:} The generative models is another approach which can be further divided into Instance and Non-Instance Generation-based methods~\cite{geng2020recent}. The former method focuses on generating useful new samples, while the second one focuses on learning the underlying distributions of the known-class data, often using Autoencoders (AEs) or Generative Adversarial Networks (GANs). The principle is that unknown samples will not fit the learned distributions, allowing for their detection based on deviation or high reconstruction error.


\section{Method / Algorithm}

\subsection{Dataset Preparation}

\paragraph{Simulate Open-Set:} Simulate an open-set scenario by holding out specific subclasses to serve as novel samples in the validation set.

\paragraph{Distribution Shift}
\begin{itemize}
    \item \textbf{Data Augmentation:} Adopt the suggestion from the brief to use data augmentation (e.g., brightness, contrast, minor geometric transforms) to improve generalization.
    \item \textbf{Validation Set:} Build a validation set for hyperparameter tuning.
\end{itemize}

\subsection{Framework}

Since it is a hierarchical OSR problem, we propose different hierarchical models.

\begin{itemize}
    \item \textbf{Model Structure:} Use a pre-trained feature extractor (e.g., CLIP~\cite{radford2021learningtransferablevisualmodels}) followed by a superclass classification head and a subclass classification head, and train the network jointly using a combined loss from both heads.
    \item \textbf{Hierarchical Constraints:} To ensure the superclass classification impacts the subclass classification, we apply a soft constraint (SE-style Feature Gating~\cite{hu2019squeezeandexcitationnetworks}) during training and a hard constraint (Masking) during inference.
\end{itemize}

\paragraph{Training with SE-style Feature Gating:}
During Training, we apply SE-style Feature Gating to recalibrate features based on their importance for superclass classification.

Given input features $\mathbf{x} \in \mathbb{R}^{d}$ from CLIP, we define:

\textit{Superclass Head:}
\begin{equation}
    \mathbf{z}^{\text{super}} = W^{\text{super}} \mathbf{x} + \mathbf{b}^{\text{super}}
\end{equation}

\textit{SE Feature Gating (Squeeze-Excitation):}
\begin{align}
    \mathbf{s} &= \sigma(W_2 \cdot \text{ReLU}(W_1 \mathbf{x})) \quad &\text{(Squeeze \& Excitation)} \\
    \tilde{\mathbf{x}} &= \mathbf{s} \odot \mathbf{x} \quad &\text{(Feature Recalibration)}
\end{align}
where $W_1 \in \mathbb{R}^{(d/r) \times d}$, $W_2 \in \mathbb{R}^{d \times (d/r)}$, $r=4$ is the reduction ratio, and $\odot$ denotes element-wise multiplication.

\textit{Subclass Head (with attended features):}
\begin{equation}
    \mathbf{z}^{\text{sub}} = W^{\text{sub}} \tilde{\mathbf{x}} + \mathbf{b}^{\text{sub}}
\end{equation}

\textit{Joint Cross-Entropy Loss:}
\begin{equation}
    \mathcal{L} = \mathcal{L}_{\text{CE}}(\mathbf{z}^{\text{super}}, y^{\text{super}}) + \mathcal{L}_{\text{CE}}(\mathbf{z}^{\text{sub}}, y^{\text{sub}})
\end{equation}

\paragraph{Inference with Hierarchical Masking:}
During inference, we apply hierarchical masking to enforce consistency between superclass and subclass predictions.

Let $\hat{c}^{\text{super}}$ be the predicted superclass and $\mathcal{S}_c$ be the set of valid subclass indices for superclass $c$. The masked subclass logits are:
\begin{equation}
    \tilde{z}^{\text{sub}}_i = 
    \begin{cases}
        z^{\text{sub}}_i & \text{if } i \in \mathcal{S}_{\hat{c}^{\text{super}}} \\
        -\infty & \text{otherwise}
    \end{cases}
\end{equation}

\subsection{Models}

\subsubsection{Threshold Scoring Methods}

These methods define a scoring function based on the outputs of a pre-trained closed-set classifier. A threshold is applied to this score to reject novel samples.

For all score-based methods, a sample is classified as novel if its score falls below a threshold calibrated on the validation set, i.e., $S < \tau$.

\subsubsection{Post-hoc Scoring with Softmax and CE Loss}

\paragraph{Threshold Scoring Methods:}
\begin{itemize}
    \item Maximum Softmax Probability (MSP)~\cite{hendrycks2018baselinedetectingmisclassifiedoutofdistribution}: The baseline approach. It uses the maximum value of the softmax distribution as the confidence score. Logit $\to$ Softmax $\to$ Threshold Gate.
    \item Out-of-Distribution Detector (ODIN)~\cite{liang2020enhancingreliabilityoutofdistributionimage}: Improves MSP by applying temperature scaling to the logits before softmax and adding small perturbations to the input image. Logit $\to$ Temperature Scaling $\to$ Softmax $\to$ Threshold Gate.
\end{itemize}

\paragraph{Maximum Softmax Probability (MSP):}
Given an input $\mathbf{x}$, the MSP score is defined as the maximum class probability:
\begin{equation}
    S_{\text{MSP}}(\mathbf{x}) = \max_{i} \frac{\exp(z_i(\mathbf{x}))}{\sum_{j=1}^{C} \exp(z_j(\mathbf{x}))}
\end{equation}

\paragraph{ODIN (with Temperature Scaling):}
ODIN applies temperature scaling $T > 1$ to the logits to separate the softmax score distributions of ID and OOD samples:
\begin{equation}
    S_{\text{ODIN}}(\mathbf{x}; T) = \max_{i} \frac{\exp(z_i(\mathbf{x})/T)}{\sum_{j=1}^{C} \exp(z_j(\mathbf{x})/T)}
\end{equation}

\paragraph{Temperature Limit Analysis:}
The effect of temperature can be understood by examining the limiting cases:

\textit{ODIN as $T \to \infty$:} As temperature increases, the softmax distribution becomes uniform, and the score approaches a linear function of the logit gap:
\begin{equation}
    \lim_{T \to \infty} S_{\text{ODIN}}(\mathbf{x}; T) \approx \frac{1}{C} + \frac{1}{C \cdot T} \left( z_{\max}(\mathbf{x}) - \bar{z}(\mathbf{x}) \right)
\end{equation}
where $z_{\max}$ is the maximum logit and $\bar{z}$ is the mean logit. This reveals that high-temperature ODIN measures the \textbf{Relative Sharpness} of the logit distribution.

\paragraph{Magnitude Information:}
A known issue with MSP and ODIN is their reliance on the Softmax function. Softmax forces output probabilities to sum to 1, losing the magnitude information of the underlying logits, which often leads to \textbf{overconfidence} on unseen data. Logit-based scoring methods in the next part resolve this problem.

\begin{center}
\includegraphics[width=\linewidth]{graph/PostHocScoreBasedMethods.png}
\end{center}

\subsubsection{Post-hoc Scoring with Sigmoid and BCE Loss}

\paragraph{Consistency between Threshold Scoring Method and Training Objective:}
In the MSP framework, the threshold scoring method (based on Softmax probabilities) and the training objective (Cross-Entropy loss based on Softmax) are consistent, as neither preserves magnitude information. However, for Logit-based OOD detection, there is a theoretical inconsistency: the threshold scoring method relies on Logits and their magnitude information, whereas the training objective (CE loss with Softmax) discards this information. Therefore, to ensure consistency and preserve magnitude information, we propose replacing Softmax with \textbf{Class-wise Sigmoids}~\cite{shu2017docdeepopenclassification}.

\paragraph{Formulation:}
Instead of the standard Softmax normalization, we apply independent sigmoid activations to each class logit, treating the multi-class classification problem as multiple \textbf{one-vs-rest} (\textbf{OvR}) binary classification problems:
\begin{equation}
    p_i = \sigma(z_i) = \frac{1}{1 + e^{-z_i}}, \quad i = 1, \ldots, C
\end{equation}
where $z_i$ is the logit for class $i$, and $C$ is the number of classes.

The training objective becomes Binary Cross-Entropy (BCE) loss with one-hot encoded targets:
\begin{equation}
    \mathcal{L}_{\text{BCE}} = -\frac{1}{C} \sum_{i=1}^{C} \left[ y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right]
\end{equation}
where $y_i \in \{0, 1\}$ is the one-hot encoded ground truth for class $i$.

\paragraph{Threshold Scoring Methods}
\begin{itemize}
    \item Maximum Sigmoid (MaxSigmoid) Score~\cite{shu2017docdeepopenclassification}: Use the maximum value of the sigmoid distribution as the confidence score. Logit $\to$ Sigmoid $\to$ Threshold Gate.
    \item Energy-based Out-of-distribution Detection~\cite{liu2021energybasedoutofdistributiondetection}: Uses the free energy derived from the logits as the scoring function. Lower energy indicates known data, while higher energy indicates novel data. Logit $\to$ Energy $\to$ Threshold Gate.
\end{itemize}

\paragraph{Maximum Sigmoid (MaxSigmoid) Score (with Temperature Scaling):}
\begin{equation}
    S_{\text{MaxSigmoid}}(\mathbf{x}; T) = \max_{i} \sigma(z_i(\mathbf{x})/T)
\end{equation}

\paragraph{Energy Score (with Temperature Scaling):}
The free energy function $E(\mathbf{x}; T)$ using temperature $T$ is defined as:
\begin{equation}
    E(\mathbf{x}; T) = -T \cdot \log \sum_{i=1}^{C} \exp(z_i(\mathbf{x})/T)
\end{equation}
Lower energy values indicate higher confidence that the sample belongs to a known class. To maintain consistency with MSP and ODIN (where higher scores indicate known classes), we define the Energy score as the negated energy:
\begin{equation}
    S_{\text{Energy}}(\mathbf{x}; T) = -E(\mathbf{x}; T) = T \cdot \log \sum_{i=1}^{C} \exp(z_i(\mathbf{x})/T)
\end{equation}

\paragraph{Temperature Limit Analysis:}
The effect of temperature can be understood by examining the limiting cases:

\textit{Energy as $T \to 0$:} As temperature decreases, the LogSumExp converges to the maximum:
\begin{equation}
    \lim_{T \to 0} E(\mathbf{x}; T) = -\max_{i} z_i(\mathbf{x})
\end{equation}
This shows that low-temperature Energy reduces to the negative \textbf{MaxLogit}, measuring the \textbf{Absolute Magnitude} of the strongest prediction.

\subsubsection{Threshold Calibration Methods}
For all score-based methods, a threshold $\tau$ must be calibrated on the validation set to distinguish known from unknown samples. We implement two calibration strategies:

\paragraph{Quantile-based Threshold (Target Recall):}
Given a target recall rate $r$ (e.g., $r = 0.95$), we set the threshold as the $(1-r)$-th quantile of known class scores:
\begin{equation}
    \tau_{\text{Quantile}} = Q_{1-r}(\{S(\mathbf{x}_i)\}_{i \in \text{known}})
\end{equation}
This guarantees exactly $r$ fraction of known samples will have scores above the threshold.

\paragraph{Z-Score Threshold:}
We model the known class score distribution and set the threshold using a multiplier $k$:
\begin{equation}
    \tau_{\text{ZScore}} = \mu - k \cdot \sigma
\end{equation}
where $\mu$ and $\sigma$ are the mean and standard deviation of known class scores. Under a Gaussian assumption, $k = 1.645$ corresponds to approximately 95\% recall.

\paragraph{Comparison:}
The Quantile method is \textbf{distribution-free} and guarantees the target recall regardless of score distribution. The ZScore method assumes approximate normality but can be more stable for small validation sets. In practice, ZScore works well for bounded scores (MSP, MaxSigmoid) but may underperform for unbounded scores (Energy).

% ================================= Not going to Implement =================================
%\subsubsection{Trainable Confidence Estimation Methods}
%Unlike post-hoc methods, these approaches modify the model architecture during training to explicitly learn representations better suited for distinguishing known from unknown.
%
%\begin{itemize}
%    \item \textbf{Auxiliary Confidence Gating~\cite{devries2018learningconfidenceoutofdistributiondetection}:} Add a parallel confidence branch alongside the main classification branch. The network is trained to output both a class prediction and a scalar confidence score via a sigmoid output. The model learns to output low confidence for samples that are difficult to classify or likely out-of-distribution.
%\end{itemize}

\subsubsection{OpenMax Hierarchical Classifier}
OpenMax~\cite{7780542} is a classic discriminative method in OSR, which is based on \textbf{Feature Distribution} (Weibull Distribution).
\begin{itemize}
    \item \textbf{Stage 1 - OpenMax for Superclass:} Use OpenMax at the superclass level, it will either classify a sample as known superclass or novel superclass.
    \item \textbf{Stage 2 - OpenMax for Subclass:} If the sample is identified as novel at the previous level, it will also be classified as novel subclass. Otherwise, we train a separate OpenMax classifier on its corresponding set of known subclasses.
\end{itemize}

\subsubsection{Hierarchical Class Anchor Clustering (CAC)}
CAC is a Discriminative method that is based on \textbf{Metric Learning}.
\begin{itemize}
    \item \textbf{Stage 1 - Hierarchical Anchor Design:} Design structured anchors~\cite{miller2021class} by first defining a unique base vector for each superclass. Subclass anchors will then be created by adding small offset vectors to their corresponding superclass base vector.
    \item \textbf{Stage 2 - Training:} Train the network using CAC loss function with hierarchical anchors forcing the network to learn a semantically meaningful logit space where the cluster layout directly mirrors problem's hierarchy.
    \item \textbf{Stage 3 - Inference:} Use the distance-based rejection process to classify known subclasses, while distinguishing rejected samples as either ``Unknown Superclass'' or ``Unknown Subclass''.
\end{itemize}

% ================================= Deprecated =================================
% \subsubsection{Probability Density Estimation + Hierarchical Classifier}

% \begin{itemize}
%     \item \textbf{Stage 1 - Probability Density-based Model for novel superclass:} Use a probability density estimation model which directly learns the probability distribution of the known dataset and addresses the OSR problem for the superclass. Data from a novel superclass should deviate from this distribution, receiving a probability score lower than a threshold, thus labeled as ``novel''.

%     \item \textbf{Stage 2 - Hierarchical Classifier for Known Superclasses:} Samples that are not identified as ``novel'' by the density model are then passed to a hierarchical classifier described in the first method, but without applying the threshold to the superclass predictor.
% \end{itemize}
% ================================= Deprecated =================================

\subsection{Evaluation}

\begin{itemize}
    \item \textbf{Primary Metric:} Our top priority is to significantly improve upon the CLIP baseline's performance on ``Unseen Accuracy''.
    \item \textbf{Secondary Metrics:} We will monitor all official metrics, including ``Overall Accuracy", ``Seen Accuracy'', and ``Categorical Cross-entropy'' for both class levels.
    \item \textbf{Threshold Tuning:} To determine the gatekeeper's probability threshold and the expert's confidence threshold, we will simulate an open-set scenario (e.g., by holding out subclasses during training) and tune on our local validation set to maximize ``Unseen Accuracy''.
\end{itemize}

\bibliographystyle{alpha}
\bibliography{reference}

\end{document}
